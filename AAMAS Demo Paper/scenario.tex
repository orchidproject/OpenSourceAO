\section{The Disaster Scenario}\label{sec:scenario}
\noindent We consider a disaster scenario in which a satellite, powered by radioactive fuel,  has crashed in a sub-urban area.\footnote{Given the invisibility of radiation, it is possible to create a believable and challenging environment for the responders to solve in our mixed-reality game (see Section \ref{sec:atomicorchid}).} Debris is strewn around a large area, damaging buildings and causing accidents and injuring civilians. Moreover, radioactive particles discharged from the debris are gradually spreading over the area, threatening to contaminate food reserves and people. Hence, emergency services (including medics, soldiers, and fire-fighters) are deployed to  evacuate the casualties and key assets (e.g., food reserves, medication, fuel), each requiring different teams of responders, before they are engulfed by the radioactive cloud.  In what follows, we describe the use of a planning agent at headquarters to help coordinate the team. 

%We then propose an algorithm to solve this optimisation problem (in Section \ref{sec:algo}). In Section \ref{sec:atomicorchid}, we demonstrate how this algorithm can be used by a software agent (in our mixed-reality game) in a mixed-initiative process to coordinate first responders. 

%\subsection{Formal Model}
%\noindent Let $G$ denote a grid overlaid on top of the disaster space, and assume the satellite debris, casualties, assets, and actors are located at various coordinates $(x,y) \in G$ in this grid. The radioactive cloud induces a radioactivity level  $l \in [0,100]$ at every point it covers (100 corresponds to maximum radiation and 0 to no radiation). While the exact radiation levels can be measured by responders on the ground (at a given location) using their geiger counter, we assume that additional information is available  from existing sensors  in the area.\footnote{This assumption is not central to our problem and only serves to inform the decision making of the agent as we see later. It is also possible to obtain similar information about radiation levels by fusing the responders' geiger counter readings, but this is beyond the scope of the paper.} However, this information is uncertain due to the poor positioning of the sensors and the variations in wind speed and direction (we show how this uncertainty is captured in the next section). A number of safe zones $G' \subseteq G$ are defined where the responders can drop off assets and casualties (i.e., \emph{targets} to be rescued). Let the set of first responders be denoted as $I = \{p_1, \cdots p_i \cdots, p_n\}$, where $|I| = n$ and the set of  targets to be rescued (i.e., rescue tasks) be denoted as  $T = \{t_1,\cdots, t_j, \cdots, t_m\}$, where $|T| = m$. A rescue task is performed by picking the target up, carrying it to a safe zone, and dropping it off.  As responders perform rescue tasks, they may become tired, get injured, or receive radiation doses that may, at worst, be life threatening. Hence, we assign each responder  a health level $h_i\in [0,100]$ that decreases based on its radiation dose ($h_i$ is decreased by $0.02 \times l$ per second given a radiation level $l$) and assume that its decision to pick up and carry the target allocated to it is liable to some uncertainty (e.g., they may not want to pick a target because it is too far away or it is unclear how long it is going to take them to  carry it back  to a safe zone).  Moreover, let $\Theta$ denote the types of responders (e.g., fire brigade, soldier, or medic)  and assume a responder's type determines the capabilities  she has and therefore the tasks  she can perform. We denote as $\theta_i \in \Theta$ the type of responder $p_i$. In turn, to complete a given task $t_j$,  a set of responders $C \subseteq I$ with specific types $\Theta_{t_j} \subseteq \Theta$ is required to pick up $t_j$. Thus, a task can only be completed by a team of responders $C_j$ if $\{\theta_i | p_i \in C_j\} = \Theta_{t_j}$. Given the distribution of responders across the physical space, different sub-teams will perform to different levels (as they have to travel different distances) and this poses a challenge for the commander and to find the best teams needed to perform the tasks.
%\begin{figure}[htbp]
%\includegraphics[width=\columnwidth]{scenario.jpg}\vspace{-5mm}
%
%\label{fig:scenario}
%\caption{The interactions between different actors in the disaster scenario. Lines represent communication links. Planner agent and coordinator sit in the headquarters (HQ). First responders (FRs) can communicate with all actors directly.}\end{figure}
\subsection{Human-Agent Collaboration}
\noindent In line with practice in many countries, we assume that the first responders are coordinated from a headquarters (HQ) headed by a human coordinator $H$. In our case, $H$ may be assisted by an autonomous task planning agent $PA$ (more details in Section \ref{sec:taskallocation}), that can receive input from, and direct, the first responders.   Both  $H$ and $PA$  can communicate their  instructions (task plans to pick up targets) directly to the responders using an instant messaging system (or walkie talkie).  While these instructions may be in natural language for $H$, $PA$ instructs them with simple requests such as ``Pick up target X at position Y with team-mates Z'' messages. In turn, the responders may not want to do some tasks (for reasons outlined above) and may therefore simply accept or reject the received instruction from $PA$ or $H$.\footnote{While some agencies may be trained to obey orders (e.g., military or fire-fighting), others (e.g., transport providers or medics) may not be trained to do so.} However, $H$ can query the responders' decisions and request  more information about their status (e.g., fatigue or health) and goals (e.g., meeting with team-mate at position X or going for task Y). Instead, if a task is rejected by the responders, $PA$ records this as a constraint on its task allocation procedure  and returns a new plan. Thus on the one hand, richer interactions are possible between $H$ and the first responders than between them and $PA$. On the other hand, $PA$ runs a sophisticated task allocation algorithm that can compute an efficient allocation, possibly better than the one computable by $H$ (particularly when many responders need to be managed).  

In our demo, the focus will not be on the algorithm run by PA, but focus on the interactions between human players and the planner agent. Hence, in the next subsection we provide a brief overview of the task allocation/path planning algorithm used by $PA$ and then go on to describe the AtomicOrchid game.
\subsection{Planning Agent Algorithm}\label{sec:taskallocation}
Previous agent-based models for team coordination in disaster response typically assume deterministic task executions and environments \cite{ramchurn:etal:2010,Scerri2005}. However, in order to evaluate agent-guided coordination in a real-world environment, it is important to consider uncertainties due to player behaviours and the environment (as discussed in the previous section). Given this, we develop a new representation for the task allocation problem in disaster response that does take into account such uncertainties. More specifically, we represent this problem using a Multi-Agent Markov Decision Process (MMDP) that captures the uncertainties of the radioactive cloud and the responders' behaviours. We model the spreading of the radioactive cloud as a random process over the disaster space and allow the actions requested from the responders to  fail (because they decline to go to a  task) or incur delays (because they are too slow) during the rescue process. Thus in the MMDP model, we represent  task executions as stochastic processes of state transitions, while the uncertainties of the radioactive cloud and the responders' behaviours can be easily captured with transition probabilities. 

At each decision step, we assume $PA$  fully observes the state of the environment by collecting sensor readings of the radioactive cloud and GPS locations of the responders. Given a policy of the MMDP, a joint action  can be selected and broadcast to the responders. We next describe the game within which $PA$ uses its algorithms to interact with human players.
%It is important to note that our model captures different types of flexible control: (i) agent-based: when $PA$ directly instructs the responders  and they can use multiple iterations of accept/reject to collaboratively converge to a definite  plan (ii) human-based: when $H$ communicates goals to the responders in open-ended interactions that may permit $H$ to gain a better understanding of the context than $PA$ and therefore formulate corrective measures faster. Moreover, in contrast to previous work that suggested \emph{transfer-of-control} regimes \cite{scerri:etal:2005}, our approach does not constrain transfers of control to target specific decision points in the operation of the system. Rather, our interaction mechanisms are designed (see Section \ref{sec:atomicorchid}) to allow human control at any point (and our results  in Section \ref{sec:evaluation} validate this approach). 

%\input{mmdp_model.tex}
