\section{Team Coordination Algorithm}
\label{sec:algo}
While the MMDP model captures the uncertainties of our
problem, it results in a very large search space making it
practically impossible to compute the optimal solution. Hence, we consider approximate solutions that still provide
high quality allocations. To this end, we decompose $PA$'s
decision-making process into a hierarchical planning process: at the top level, a {\em task planning} algorithm is run
for the whole team to assign the best task to each responder given
the current state of the world; at the lower level, given a task, a
{\em path planning} algorithm is run by each responder to find the
best path to the task from her current location. Furthermore,
since not all states of MMDPs are relevant to the problem, we
only need to consider the reachable states given the current state.
Hence, we compute the policy online, starting from the current state
of the problem. This reduces computation significantly
because the number of the reachable states is usually much smaller
than the overall state space. In what follows, we describe each
level of our {\em online hierarchical planning} algorithm.

\subsection{Task Planning} \label{sec:taskplanning} As described earlier, each responder $p_i$ is of a specific type
$\theta_i \in \Theta$ that determines which task she can perform
and  a task $t$ can only be completed by a team of responders with
the required types $\Theta_t$. If, during the execution
of a plan, a responder $p_i$ is incapable of performing a task
(e.g., because she is tired or unclear where the task is), she
is removed from the set of responders under consideration
(that is $I \to I \setminus p_i$). This information can be obtained
from the state $s \in S$. When a task $t$ is completed by a chosen
team, the task is simply removed from the set (that is $T \to
T\setminus t$ if $t$ has been completed).

Now, to capture the efficiency of groupings of responders at
performing tasks, we define the value of a team $v(C_{jk})$ that
reflects the level of performance of team $C_k$ in performing task
$t_j$. This is computed from the estimated rewards that the team
obtains for performing $t_j$.  Then, the goal of the task planning
algorithm is to assign a task to each team that maximises the
overall team performance given the current state $s$, i.e.,
$\sum_{j=1}^m v(C_{j})$ where $C_j$ is a team for task $t_j$ and
$\{ C_1, \cdots, C_m \}$ is a {\em partition} of $I$ ($\forall
j\neq j', C_j \bigcap C_{j'} = \emptyset$ and $\bigcup_{j=1}^m
C_j=I$). In what follows, we first detail the procedure to compute
the value of all teams that are valid in a given state and then
detail the task allocation algorithm.


\subsubsection{Team Value Calculation.}
The computation of  $v(C_{jk})$ for each team $C_{jk}$ is
challenging since there are many ways to configure teams and this needs to be done repeatedly (since there are more tasks than responders).
Moreover, a new policy must be computed after a given task $t_j$ is completed. This is time-consuming given the number of
states and joint actions. Hence, we propose to estimate
$v(C_{jk})$ through several simulations. This saves
computation as it avoids computing the complete policy to come
up with a good estimate of the team value, though we may not be
able to evaluate all possible future outcomes. According to the
law of large numbers, if the number of simulations is sufficiently
large, the estimated value will converge to the true $v(C_{jk})$.

Specifically, in each simulation, we first assign the responders in
$C_{jk}$ to task $t_j$ and run the simulator starting from the
current state $s$. After task $t_j$ is completed, the simulator
returns the sum of the rewards $r$ and the new state $s'$. If all
the responders in $C_{jk}$ are incapable of doing other tasks
(e.g., because they suffered radiation burns), the simulation is terminated. Otherwise, we estimate the expected value of $s'$ using Monte-Carlo
Tree Search (MCTS)~\cite{kocsis2006bandit}, which provides a good
trade-off between exploitation and exploration of the policy space
and has been shown to be efficient for large MDPs.\footnote{Other
methods such as sequential greedy assignment or swap-based hill
climbing~\cite{proper2009solving} may also be useful. However, they
do not explore the policy space as well as MCTS.} After $N$
simulations, the average value is returned as an approximation of
the team value.

\subsubsection{Coordinated Task Allocation.}
Given the team values computed above, we then solve the following
optimisation problem to find the best solution:
\begin{equation}
  \begin{array}{lll}
    \max\limits_{x_{jk}} & \sum_{j, k} x_{jk} \cdot v(C_{jk}) & \\[2pt]
    \mbox{s.t.} & x_{jk} \in \{0, 1\} & \\[2pt]
    & \forall j, \sum_{k} x_{jk} \leq 1 & \mbox{(i)} \\[2pt]
    & \forall i, \sum_{j, k} \delta_i(C_{jk}) \leq 1 & \mbox{(ii)}
  \end{array}
  \label{eq:cf}
\end{equation}
where $x_{jk}$ is the boolean variable to indicate whether team
$C_{jk}$ is selected for task $t_j$ or not, $v(C_{jk})$ is the
value of team $C_{jk}$, and $\delta_i(C_{jk}) = 1$ if responder
$p_i\in C_{jk}$ and 0 otherwise. In the optimisation, constraint
(i) ensures that a task $t_j$ is allocated to at most one team (a
task does not need more than one group of responders) and
constraint (ii) ensures that a responder $p_i$ is assigned to only
one task (a responder cannot do more than one task at the same
time). This is a standard {\em mixed integer linear program} that
can be efficiently solved using solvers (e.g., CPLEX or LP-Solve).

\subsubsection{Adapting to Responder Requests.}\label{sec:adaptive}
An important characteristic of our approach is that it can easily
incorporate the preferences of the responders. For example, if a
responder declines a task allocated to it by the planning agent, we
simply filter out the teams for the task that contain this
responder. By so doing, the responder will not be assigned to the
task. Moreover, if a responder prefers to do the tasks with another
responder, we can increase the weights of the teams that contain
them in Equation~\ref{eq:cf} (by default, all teams have identical
weights of 1.0). Thus, our approach is adaptive to the
 preferences of human responders.\vspace{-2mm}

\subsection{Path Planning}
\label{sec:pathplanning}
In the path planning phase, we compute the best path for a
responder to her assigned task. This phase is stochastic due to uncertainties in the radioactive cloud and the responders'
actions. We model this problem as a single-agent MDP that can be
solved by many existing solvers. Among them, we choose {\em
real-time dynamic programming} (RTDP)~\cite{barto1995learning}
because it is simple and particularly fits our problem (that is, a goal-directed MDP with large number of states). However, other
approaches for solving large MDPs  could equally be used here.

\subsection{Simulation Results}
While our main focus  is the evaluation of $PA$ in the real-world, it is important  to ensure it
performs acceptably. There are no state of the art methods to benchmark against given that other approaches assume deterministic task execution and environments \cite{koes2006constraint,ramchurn:etal:2010b,Scerri2005,Chapman2009}. However, it should be clearly better than simple alternatives such as  greedy or myopic methods.  In
the greedy method, the responders are uncoordinated and simply select the
closest tasks they can do. In the myopic method, the responders are
coordinated to  select the tasks but have no lookahead for
future tasks. For each method, we use our
path planning algorithm to compute the path for each responder. 

Table~\ref{tab:simulation} shows the results for a problem with 17
tasks and 8 responders on a 50$\times$55 grid. As can be seen, our
MMDP algorithm completes more tasks than the myopic and greedy
methods. More importantly, our
algorithm guarantees the safety of the responders (i.e., by avoiding the radioactive cloud), while in the
myopic method  only 25\% of the responders survive and in the
greedy method none survived.
More extensive evaluations are not reported here because
our focus  is on the use of the algorithm in a field deployment
to test how humans take up advice computed by the planning agent
$PA$ as we show next.\vspace{-2mm}
\begin{table}[htbp]
\begin{center}\small
  
  \begin{tabular}{l|c|c|c}
   & MMDP & myopic & greedy \\
  \hline
  \#completed tasks & 71\% & 65\% & 41\% \\
  \hline
  \#responders alive at the end & 100\% & 25\% & 0\% \\
  \end{tabular}
  \end{center}\caption{Simulation results for MMDP, myopic, and greedy.}
  \label{tab:simulation}\vspace{-4mm}
\end{table}
